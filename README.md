Aqui est√° o `README.md` completo com as atualiza√ß√µes incluindo o uso do modelo **deepseek-r1:8b** no Ollama server:

---

# repo-insight-bot

**repo-insight-bot** √© uma aplica√ß√£o que utiliza intelig√™ncia artificial para gerar insights sobre reposit√≥rios do GitHub. Ele processa dados de reposit√≥rios, gera embeddings utilizando Sentence Transformers, armazena os dados no QdrantDB, e permite consultas interativas para responder perguntas sobre o reposit√≥rio.

---

## **Recursos**

- **An√°lise de Reposit√≥rios GitHub**: Processa dados do reposit√≥rio e extrai informa√ß√µes relevantes.
- **Gera√ß√£o de Embeddings**: Utiliza Sentence Transformers para criar embeddings sem√¢nticos de texto.
- **Armazenamento Vetorial**: Armazena embeddings no QdrantDB para consultas eficientes.
- **Integra√ß√£o com Ollama**: Suporte opcional para modelos generativos locais, como **LLaMA** ou **deepseek-r1:8b**.

---

## **Configura√ß√£o do Projeto**

### **Depend√™ncias**

Certifique-se de ter as seguintes ferramentas instaladas:

- **Docker**
- **Docker Compose**
- **Python** (vers√£o 3.10.12)
- **Poetry** (para gerenciar depend√™ncias Python)

### **Instala√ß√£o**

1. Clone o reposit√≥rio:
   ```bash
   git clone https://github.com/GoLogann/repo-insight-bot.git
   cd repo-insight-bot
   ```

2. Instale as depend√™ncias do projeto usando o Poetry:
   ```bash
   poetry install
   ```

3. Configure os cont√™ineres Docker:
   ```bash
   sudo docker compose -f docker-compose.yml -p repo-insight-bot up -d
   ```

   Esse comando iniciar√° os servi√ßos:
   - **Qdrant**: Banco de dados vetorial para armazenar embeddings.
   - **Ollama** (opcional): Para usar modelos locais como LLaMA ou **deepseek-r1:8b**.

4. **Configura√ß√£o do Modelo deepseek-r1:8b no Ollama**:
   Para utilizar o modelo **deepseek-r1:8b** no Ollama, execute o seguinte comando no terminal do cont√™iner do Ollama:
   ```bash
   docker exec -it <nome_do_container_ollama> ollama run deepseek-r1:8b
   ```
   Substitua `<nome_do_container_ollama>` pelo nome do cont√™iner do Ollama em execu√ß√£o.

---

## **Como Usar**

### **1. Endpoint para perguntas**

O endpoint principal √©:

```http
POST /repo-insight-bot/ask
```

**Request Body:**

```json
{
  "repo_url": "https://github.com/GoLogann/phishing-quest-api",
  "question": "Qual √© o objetivo deste reposit√≥rio?"
}
```

**Exemplo de Resposta:**

```json
{
  "response": "O reposit√≥rio √© uma API para um game estilo quizz"
}
```

---

### **Notas Adicionais**

- O modelo **deepseek-r1:8b** √© uma op√ß√£o poderosa para gerar respostas contextualizadas e pode ser integrado ao Ollama para uso local.
- Certifique-se de que o cont√™iner do Ollama esteja em execu√ß√£o antes de configurar o modelo.

---

Se precisar de mais ajustes ou tiver d√∫vidas, √© s√≥ avisar! üòä